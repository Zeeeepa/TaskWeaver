"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[803],{2596:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>h});var a=t(4848),i=t(8453);const s={title:"Vision input for the Planner",authors:"liqli",date:new Date("2025-03-13T00:00:00.000Z")},o=void 0,r={permalink:"/TaskWeaver/blog/vision",editUrl:"https://github.com/microsoft/TaskWeaver/tree/main/website/blog/vision.md",source:"@site/blog/vision.md",title:"Vision input for the Planner",description:"Introduction",date:"2025-03-13T00:00:00.000Z",tags:[],readingTime:2.36,hasTruncateMarker:!0,authors:[{name:"Liqun Li",url:"https://liqul.github.io",title:"Principal Researcher",imageURL:"https://liqul.github.io/assets/logo_small_bw.png",key:"liqli",page:null}],frontMatter:{title:"Vision input for the Planner",authors:"liqli",date:"2025-03-13T00:00:00.000Z"},unlisted:!1,nextItem:{title:"What makes a good agent reasoning framework?",permalink:"/TaskWeaver/blog/reasoning"}},l={authorsImageUrls:[void 0]},h=[{value:"Introduction",id:"introduction",level:2},{value:"How vision input is supported in TaskWeaver",id:"how-vision-input-is-supported-in-taskweaver",level:2},{value:"An example",id:"an-example",level:2},{value:"Extension",id:"extension",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",img:"img",mermaid:"mermaid",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"We have supported vision input for the Planner role in TaskWeaver.\nThe Planner role is responsible for generating the high-level plan for the task.\nThe vision input is a new type of input that contains images.\nThis feature is useful when the task requires visual understanding."}),"\n",(0,a.jsx)(n.h2,{id:"how-vision-input-is-supported-in-taskweaver",children:"How vision input is supported in TaskWeaver"}),"\n",(0,a.jsxs)(n.p,{children:["In TaskWeaver, we added a new role called ",(0,a.jsx)(n.code,{children:"ImageReader"})," to read images and provide the image url (for remote images) or\nthe image encoded in base64 (for local images) to the Planner role.\nTo have this new role, you need to include it in the project configure file as follows:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n    "session.roles": [\n        "planner",\n        "code_interpreter",\n        "image_reader"\n    ]\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The ImageReader role takes the path or the url of the image as input and prepares a response Post for the Planner role. As described ",(0,a.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest",children:"here"})," for Azure OpenAI API, if the image is local, ImageReader need to encode the image in base64 and pass it to the API. If the image is remote, ImageReader need to provide the url of the image.\nThe Planner role can then use the image information for various tasks."]}),"\n",(0,a.jsx)(n.h2,{id:"an-example",children:"An example"}),"\n",(0,a.jsx)(n.p,{children:"Let's ask the agent to describe any uploaded image."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"image_reader",src:t(416).A+"",width:"789",height:"2757"})}),"\n",(0,a.jsx)(n.p,{children:"The flow of the conversation is as follows:"}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\n    User --image path--\x3e Planner\n    Planner --image path--\x3e ImageReader\n    ImageReader --image encoded in Base64--\x3e Planner\n    Planner --response--\x3e User"}),"\n",(0,a.jsxs)(n.p,{children:["In the example above, the User talks to the agent in Web UI and uploads an image.\nTaskWeaver also support providing the image path in console mode, either using the ",(0,a.jsx)(n.code,{children:"/load"})," command or just include\nthe image path in the input message."]}),"\n",(0,a.jsx)(n.h2,{id:"extension",children:"Extension"}),"\n",(0,a.jsx)(n.p,{children:"If you look into the implementation of the ImageReader role, you will find that it is quite simple.\nThe key logic is shown in the following code snippet:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'if image_url.startswith("http"):\n    image_content = image_url\n    attachment_message = f"Image from {image_url}."\nelse:\n    if os.path.isabs(image_url):\n        image_content = local_image_to_data_url(image_url)\n    else:\n        image_content = local_image_to_data_url(os.path.join(self.session_metadata.execution_cwd, image_url))\n    attachment_message = f"Image from {image_url} encoded as a Base64 data URL."\n\npost_proxy.update_attachment(\n    message=attachment_message,\n    type=AttachmentType.image_url,\n    extra={"image_url": image_content},\n    is_end=True,\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["After the image url is obtained, the ImageReader role will encode the image in base64 if the image is local.\nThen, it will create an attachment in the response Post and pass the image content to the Planner role.\nTo achieve this, the attachment is created with the type ",(0,a.jsx)(n.code,{children:"AttachmentType.image_url"})," and the image content is\npassed as extra data with the key ",(0,a.jsx)(n.code,{children:"image_url"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Therefore, if we want to support other scenarios with vision input, we can extend the ImageReader role by adding more logic\nto handle different types of contents. One example is to support reading a document with text and images.\nWe can add an attachment for each image in the document and pass the list of attachments to the Planner role."})]})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},416:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/image_reader-88cc65f5b9b2195a49673c046f0f27d5.png"},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);